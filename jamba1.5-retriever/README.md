# Jamba Retriever: Fine-Tuning Jamba Models for Embedding Generation

This notebook demonstrates how to fine-tune a jamba-based model (`ai21labs/Jamba-tiny-dev`) using a contrastive loss function for embedding generation. The notebook guides you through a step-by-step process of training the model using a dataset of sentence pairs and their similarity scores. After training, the model will be able to generate embeddings where similar sentences are closer in vector space, and dissimilar sentences are farther apart. Both model and dataset ara available in Huggingface here:

    **Model: ai21labs/Jamba-tiny-dev : https://huggingface.co/ai21labs/Jamba-tiny-dev
    **Dataset: PhilipMay/stsb_multi_mt: https://huggingface.co/datasets/PhilipMay/stsb_multi_mt

## Why Jamba is Efficient for RAG Use Cases:

Jamba, with its hybrid MAMBA-transformer architecture, addresses some of the inefficiencies of traditional transformers by focusing on reducing computational overhead, making it faster and more efficient. This makes it an ideal candidate for **RAG workflows**, where generating embeddings quickly and efficiently is crucial for real-time retrieval tasks. By leveraging this efficiency, embedding models based on Jamba can perform high-quality similarity searches and retrieval tasks with reduced latency, benefiting use cases like chatbots, search engines, and document retrieval systems.

## The notebook covers the following steps:

1. **Step 0: Install Necessary Dependencies**  
   _Goal:_ To ensure that the required Python libraries are available to load datasets, process text, train models, and interact with AWS SageMaker for deployment. This step prepares the environment for all subsequent operations.

2. **Step 1: Dataset Preparation**  
   _Goal:_ To load and preprocess the dataset (`PhilipMay/stsb_multi_mt`), which contains pairs of sentences and their similarity scores. Preprocessing involves tokenizing the text so that it can be fed into the model during training.

3. **Step 2: Model Definition and Tokenizer Setup**  
   _Goal:_ The `ai21labs/Jamba-tiny-dev` model is pre-trained but needs to be fine-tuned for our specific task. We define the model architecture and the tokenizer that converts raw text into tokenized inputs required by the model.

4. **Step 3: Define the Contrastive Loss Function**  
   _Goal:_ To teach the model to push embeddings of similar sentences closer in vector space and dissimilar sentences farther apart. The contrastive loss function is crucial for learning high-quality embeddings for similarity tasks.

5. **Step 4: Custom Data Collator**  
   _Goal:_ Sentence lengths vary across the dataset, so we need to dynamically pad sentences during training. The custom data collator handles padding to ensure all sentences in a batch are the same length, allowing efficient processing by the model.

6. **Step 5: Training the Model**  
   _Goal:_ Fine-tuning adjusts the model's parameters so that it learns to generate useful embeddings for our task. The model will optimize based on the contrastive loss function to generate better embeddings for similar and dissimilar sentences.

7. **Step 6: Save Model to S3**  
   _Goal:_ To securely store the fine-tuned model in Amazon S3 for future use in inference or further fine-tuning. This ensures that the model is preserved beyond the current session.

8. **Step 7: Deploy the Model to SageMaker**  
   _Goal:_ To make the model accessible for real-time inference, we deploy it to a SageMaker endpoint. This allows external applications to send requests to the model and get embeddings in return.

9. **Step 8: Test the Model with Similar and Dissimilar Sentences**  
   _Goal:_ To verify that the model is performing as expected by sending test sentences through the deployed endpoint. We check that the model generates embeddings where similar sentences are mapped closer together in the vector space.

10. **Step 9: Compute Distance and Plot Results**  
    _Goal:_ To compute the cosine similarity between sentence embeddings and visualize the results. This step helps validate that the embeddings generated by the model accurately reflect sentence similarity and allows us to visually assess the model's performance with color-coded results.

---

At the end of this notebook, you will have a fully trained and deployed transformer model capable of generating high-quality embeddings, which can be used for tasks such as retrieval-augmented generation (RAG) workflows and sentence similarity searches. Please note that for the purposes of showcasing these capabilities via SageMaker and The Huggingface Inference Toolkit we have chosen the Dev version of Jamba1.5. which does not take long to download and train and can be deployed on smaller GPU instances
