{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for LLM Pre-training\n",
    "## Notebook 2: Data Preprocessing\n",
    "\n",
    "In this notebook we will setup a data preprocessing pipeline to prepare the data for LLM pre-training. We will use the [datatrove](https://github.com/huggingface/datatrove) library from Hugging Face to create a data processing pipeline. The pipeline will be implemented using [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html). We'll utilize the [step decorator](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator-create-pipeline.html) to simplify the creation of the pipeline steps.\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px; background-color: #ffffcc; color: black;\">\n",
    "<strong>Note:</strong> Make sure to fully run the first notebook to obtain the data needed for this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqq datatrove[all]\n",
    "%pip install -Uqq nltk\n",
    "%pip install -Uqq sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker import image_uris\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = (\n",
    "    sagemaker.session.Session()\n",
    ")  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "bucket = sess.default_bucket()  # default bucket name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "# get the image uri that will be used to run the pipeline\n",
    "# use pytorch image as it is the most fully featured image\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    image_scope=\"training\",\n",
    "    region=region,\n",
    "    version=\"2.3\",\n",
    "    py_version=\"py311\",\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    ")\n",
    "\n",
    "s3_data_path = json.loads(Path(\"s3_path.json\").open(\"r\").read())[\n",
    "    \"s3_path\"\n",
    "]  # read the s3 path from the file generated by the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure pipeline using @step Decorator\n",
    "By using the `@step` decorator, we can configure our pipeline as python functions that we can test locally. Then by decorating each function with `@step`, we can define the pipeline DAG that will run each step as a SageMaker job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.executor.local import LocalPipelineExecutor\n",
    "from datatrove.pipeline.extractors import Trafilatura\n",
    "from datatrove.pipeline.readers import WarcReader\n",
    "from datatrove.pipeline.writers.jsonl import JsonlWriter\n",
    "from datatrove.pipeline.filters import (\n",
    "    GopherQualityFilter,\n",
    "    GopherRepetitionFilter,\n",
    "    LanguageFilter,\n",
    "    URLFilter,\n",
    ")\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the pipeline is adopted from the example here [here](https://github.com/huggingface/datatrove/blob/main/examples/process_common_crawl_dump.py) to process crawled data. The pipeline will read the warc files, filter out unwanted URLs (there are none in our example), extract the text from the HTML using the [trafilatura library](https://trafilatura.readthedocs.io/en/latest/), apply repetition filters to remove documents with repeated content, apply additional quality filters (minimum number of words, average length of word, ration of alphabetic words, etc.), and finally save the processed data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"initial_filter\",\n",
    "    keep_alive_period_in_seconds=300,\n",
    "    image_uri=image_uri,\n",
    "    instance_type=\"ml.m5.2xlarge\",        # instance type for the pipeline\n",
    "    instance_count=1,\n",
    "    dependencies=\"requirements.txt\",     # dependencies for the pipeline\n",
    ")\n",
    "def initial_filter(dataset_name: str, s3_data_path: str, s3_output_path: str):\n",
    "    \n",
    "\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "    executor = LocalPipelineExecutor(       # LocalPipelineExecutor is used to run the pipeline on a single machine. There is a SLURM executor available for running the pipeline on a cluster \n",
    "        pipeline=[\n",
    "            WarcReader(\n",
    "                s3_data_path,\n",
    "                glob_pattern=\"*warc.gz\",  # we want the warc files\n",
    "                default_metadata={\"name\": dataset_name},\n",
    "            ),\n",
    "            URLFilter(\n",
    "                exclusion_writer=JsonlWriter(\n",
    "                    f\"{s3_output_path}/removed/url/{dataset_name}\"\n",
    "                )\n",
    "            ),\n",
    "            Trafilatura(favour_precision=True),\n",
    "            LanguageFilter(\n",
    "                exclusion_writer=JsonlWriter(\n",
    "                    f\"{s3_output_path}/non_english/\",\n",
    "                    output_filename=\"${language}/\" + dataset_name + \"/${rank}.jsonl.gz\",\n",
    "                )\n",
    "            ),\n",
    "            GopherRepetitionFilter(\n",
    "                exclusion_writer=JsonlWriter(\n",
    "                    f\"{s3_output_path}/removed/repetitive/{dataset_name}\"\n",
    "                )\n",
    "            ),\n",
    "            GopherQualityFilter(\n",
    "                exclusion_writer=JsonlWriter(\n",
    "                    f\"{s3_output_path}/removed/quality/{dataset_name}\"\n",
    "                )\n",
    "            ),\n",
    "            JsonlWriter(f\"{s3_output_path}/output/{dataset_name}\"),\n",
    "        ],\n",
    "        tasks=4,\n",
    "        logging_dir=f\"{s3_output_path}/logs/base_processing/{dataset_name}\",\n",
    "    )\n",
    "\n",
    "    executor.run()\n",
    "\n",
    "    final_output_path = f\"{s3_output_path}/output/{dataset_name}\"\n",
    "    \n",
    "    return final_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to deduplicate the filtered data. Minhashing is used to create a signature for each document, and then Locality Sensitive Hashing (LSH) is used to cluster similar documents and finally filter out duplicates by taking one document from each cluster.\n",
    "The step is adopted from the example [here](https://github.com/huggingface/datatrove/blob/main/examples/minhash_deduplication.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.pipeline.dedup import MinhashDedupSignature\n",
    "from datatrove.pipeline.dedup.minhash import (\n",
    "    MinhashConfig,\n",
    "    MinhashDedupBuckets,\n",
    "    MinhashDedupCluster,\n",
    "    MinhashDedupFilter,\n",
    ")\n",
    "from datatrove.pipeline.readers import JsonlReader\n",
    "from datatrove.pipeline.tokens import TokensCounter\n",
    "from datatrove.pipeline.writers.jsonl import JsonlWriter\n",
    "from datatrove.utils.hashing import HashConfig\n",
    "from datatrove.utils.typeshelper import Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"deduplication\",\n",
    "    keep_alive_period_in_seconds=300,\n",
    "    image_uri=image_uri,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    dependencies=\"requirements.txt\",\n",
    ")\n",
    "\n",
    "def deduplicate(s3_input_path: str):\n",
    "    \n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "    minhash_config = MinhashConfig(\n",
    "        hash_config=HashConfig(precision=64),\n",
    "        num_buckets=14,\n",
    "        hashes_per_bucket=8,\n",
    "    )  \n",
    "\n",
    "    input_reader = JsonlReader(s3_input_path)\n",
    "    s3_output_path = f\"{os.path.dirname(s3_input_path)}/deduplication\"\n",
    "\n",
    "    stage1 = LocalPipelineExecutor(\n",
    "        pipeline=[\n",
    "            input_reader,\n",
    "            MinhashDedupSignature(\n",
    "                output_folder=f\"{s3_output_path}/signatures\",\n",
    "                config=minhash_config,\n",
    "                language=Languages.english,\n",
    "            ),\n",
    "        ],\n",
    "        tasks=4,\n",
    "    )\n",
    "\n",
    "    stage2 = LocalPipelineExecutor(\n",
    "        pipeline=[\n",
    "            MinhashDedupBuckets(\n",
    "                input_folder=f\"{s3_output_path}/signatures\",\n",
    "                output_folder=f\"{s3_output_path}/buckets\",\n",
    "                config=minhash_config,\n",
    "            ),\n",
    "        ],\n",
    "        tasks=minhash_config.num_buckets,\n",
    "        depends=stage1,\n",
    "    )\n",
    "\n",
    "    stage3 = LocalPipelineExecutor(\n",
    "        pipeline=[\n",
    "            MinhashDedupCluster(\n",
    "                input_folder=f\"{s3_output_path}/buckets\",\n",
    "                output_folder=f\"{s3_output_path}/remove_ids\",\n",
    "                config=minhash_config,\n",
    "            ),\n",
    "        ],\n",
    "        tasks=1,\n",
    "        depends=stage2,\n",
    "    )\n",
    "\n",
    "    stage4 = LocalPipelineExecutor(\n",
    "        pipeline=[\n",
    "            input_reader,\n",
    "            TokensCounter(),\n",
    "            MinhashDedupFilter(\n",
    "                input_folder=f\"{s3_output_path}/remove_ids\",\n",
    "                exclusion_writer=JsonlWriter(f\"{s3_output_path}/removed\"),\n",
    "            ),\n",
    "            JsonlWriter(output_folder=f\"{s3_output_path}/deduplicated_output\"),\n",
    "        ],\n",
    "        tasks=4,\n",
    "        depends=stage3,\n",
    "    )\n",
    "    \n",
    "    stage4.run()\n",
    "    \n",
    "    final_output = f\"{s3_output_path}/deduplicated_output\"\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can combine the two steps into a single pipeline. The pipeline will take 3 parameters:\n",
    "- The location of the crawled data (warc files)\n",
    "- The name of the dataset to be created\n",
    "- The output location for the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline parameters\n",
    "s3_source_param = ParameterString(name=\"s3_source_path\")          # s3 path to the source data\n",
    "dataset_name = ParameterString(name=\"dataset_name\")               # name of the dataset to create a folder in the s3 bucket\n",
    "s3_filtered_param = ParameterString(name=\"s3_filtered_path\")      # s3 path to the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline definition\n",
    "pipeline_name = \"PreTrainDatePrep\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    steps=[deduplicate(initial_filter(dataset_name, s3_source_param, s3_filtered_param))],\n",
    "    parameters=[dataset_name, s3_source_param, s3_filtered_param],\n",
    ")\n",
    "\n",
    "# update or create the pipeline\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path = f\"s3://{bucket}/pre-training-data/\"\n",
    "\n",
    "# start the pipeline\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"dataset_name\": \"aws-blogs\",\n",
    "        \"s3_source_path\": s3_data_path,\n",
    "        \"s3_filtered_path\": s3_output_path,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the pipeline to finish\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pipeline is done running, we can get the returned value of the `deduplication` step which will contain the S3 location of the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_location = execution.result(step_name=\"deduplication\")\n",
    "print(f\"Final data location: {final_data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files in the final data location\n",
    "!aws s3 ls $final_data_location/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open one of the output files to see the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(f\"{final_data_location}/00000.jsonl.gz\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook we have created a data preprocessing pipeline using SageMaker Pipelines and the datatrove library. The pipeline processed the crawled data, filtered out unwanted URLs, extracted text from HTML, applied repetition filters, and deduplicated the data. The processed data is saved to S3 and can be used for LLM pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
