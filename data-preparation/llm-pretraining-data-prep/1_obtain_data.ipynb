{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for LLM Pre-training\n",
    "## Notebook 1: Data Collection\n",
    "\n",
    "In this notebook we will collect the data that we will use as an example for the pre-processing pipeline that will be covered in the subsequent notebook. \n",
    "\n",
    "Pre-training data can come from a wide variety of sources, including books, articles, websites, and more. In this notebook, we will source our data from [AWS Blogs](https://aws.amazon.com/blogs/). The steps we will follow are as follows:\n",
    "- Choose a blog category to scrape such as Machine Learning, Security, Big Data, etc.\n",
    "- Crawl the blog category to get the URLs of all the blog posts.\n",
    "- Scrape the content of each blog post.\n",
    "- Save the data into a [Web Archive (WARC)](https://en.wikipedia.org/wiki/WARC_(file_format)) file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries for web scraping\n",
    "%pip install -Uqq beautifulsoup4\n",
    "%pip install -Uqq requests\n",
    "%pip install -Uqq warcio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping AWS Blogs\n",
    "AWS Blogs are a great source of technical content and are also quite easy to scrape. We will use the `requests` and `beautifulsoup4` libraries to scrape the blog posts. `requests` will be used to get the HTML content of the blog posts, and `beautifulsoup4` will be used to parse the HTML content and extract the text. Each blog category has a navigation page with a url format like `https://aws.amazon.com/blogs/<category>/page/<page_number>`. Additional url paths can be appended to the base url to further filter down the content. For example, the url `https://aws.amazon.com/blogs/big-data/category/industries/financial-services/page/20/` will bring up the 20th navigation page with links to Big Data blog posts related to the financial services industry.\n",
    "\n",
    "This format of the AWS Blogs website makes it easy to scrape the blog posts. Below we define a number of functions to:\n",
    "1. Traverse the navigation pages of a given blog category\n",
    "2. Extract the URLs of the blog posts from each navigation page\n",
    "3. Scrape the full HTML content of each blog post (We'll later extract the main text from this HTML content)\n",
    "4. Save the scraped data into a WARC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from warcio.warcwriter import WARCWriter\n",
    "from warcio.statusandheaders import StatusAndHeaders\n",
    "import sagemaker\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "SAGEMAKER_SESSION = sagemaker.Session()\n",
    "S3_BUCKET = SAGEMAKER_SESSION.default_bucket()\n",
    "S3_PREFIX = 'aws-blogs-pretrain'\n",
    "\n",
    "def fetch_blog_nav_page(blog_category, page_num, industry=None):\n",
    "    \n",
    "    \"get the content of a blog navigation page\"\n",
    "    \n",
    "    url = f\"https://aws.amazon.com/blogs/{blog_category}/\"\n",
    "    \n",
    "    if industry:\n",
    "        url = f\"{url}category/industries/{industry}/\"\n",
    "    \n",
    "    if page_num != 1:\n",
    "        url = f\"{url}page/{page_num}/\"\n",
    "        \n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "def parse_blog_links(html_content):\n",
    "    \n",
    "    \"Get links to blog posts from the blog navigation page\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = []\n",
    "    for h2_tag in soup.find_all('h2', class_='blog-post-title'):\n",
    "        a_tag = h2_tag.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            links.append(a_tag['href'])\n",
    "    return links\n",
    "\n",
    "\n",
    "def fetch_blog_content(url):\n",
    "    \"get content of a blog post\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    try: \n",
    "        response.raise_for_status()\n",
    "    except:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return None\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def save_to_warc(response, writer):\n",
    "    \n",
    "    \"save crawled results to WARC file\"\n",
    "    \n",
    "    headers_list = [(k, v) for k, v in response.headers.items()]\n",
    "    status_line = f\"HTTP/1.1 {response.status_code} {response.reason}\"\n",
    "    http_headers = StatusAndHeaders(status_line, headers_list, protocol='HTTP/1.1')\n",
    "    payload = BytesIO(response.content)\n",
    "    record = writer.create_warc_record(response.url, 'response', payload=payload, http_headers=http_headers)\n",
    "    writer.write_record(record)\n",
    "\n",
    "\n",
    "def crawl_aws_blogs(start_page, blog_category, industry=None, warc_file_path=None):\n",
    "    \n",
    "    \"main function to crawl AWS blogs\"\n",
    "    \n",
    "    if warc_file_path is None:\n",
    "        warc_file_path = f\"aws_{blog_category}_blogs.warc.gz\"\n",
    "    with open(warc_file_path, 'wb') as warc_file:\n",
    "        writer = WARCWriter(warc_file, gzip=True)\n",
    "        \n",
    "        page_num = start_page\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            print(f\"Crawling page {page_num}\")\n",
    "            try:\n",
    "                html_content = fetch_blog_nav_page(blog_category, page_num, industry)\n",
    "                page_num += 1\n",
    "            \n",
    "            except requests.HTTPError as e:\n",
    "                if e.response.status_code == 404:\n",
    "                    print(f\"Page {page_num} not found. Exiting\")\n",
    "                    break\n",
    "            \n",
    "            blog_links = parse_blog_links(html_content)\n",
    "            \n",
    "            for blog_link in blog_links:\n",
    "                print(f\"Fetching blog {blog_link}\")\n",
    "                response = fetch_blog_content(blog_link)\n",
    "                if response:\n",
    "                    save_to_warc(response, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrape Machine Learning and Big Data blog posts from AWS Blogs. To add some duplicate data, we'll also scrape the Machine Learning blogs related to the financial services industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_path = Path(\"scraped_data\")\n",
    "\n",
    "if not scraped_data_path.exists():\n",
    "    scraped_data_path.mkdir()\n",
    "\n",
    "# crawl ML Blogs\n",
    "crawl_aws_blogs(1, \"machine-learning\", warc_file_path=f\"{scraped_data_path}/ml-blogs.warc.gz\")\n",
    "\n",
    "# crawl Big Data Blogs\n",
    "crawl_aws_blogs(1, \"big-data\", warc_file_path=f\"{scraped_data_path}/big-data-blogs.warc.gz\")\n",
    "\n",
    "# crawl ML Financial Services Blogs\n",
    "crawl_aws_blogs(1, \"machine-learning\",  \"financial-services\", warc_file_path=f\"{scraped_data_path}/ml-fsi-blogs.warc.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data scraped, we can upload it to S3 and proceed to the next notebook where we will pre-process the data for LLM pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = SAGEMAKER_SESSION.upload_data(path=str(scraped_data_path), bucket=S3_BUCKET, key_prefix=S3_PREFIX)\n",
    "\n",
    "# save the S3 path to a file for use in later notebook\n",
    "Path(\"s3_path.json\").write_text(json.dumps({\"s3_path\": s3_path}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
